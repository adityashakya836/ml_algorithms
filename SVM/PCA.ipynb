{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset:\n",
    "You will be working with two datasets: Train and Test, both containing images of 40 individuals. Each dataset has been preprocessed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PCA function that takes two inputs: dataset, and k= number of principle component, and return the transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca(Data,k):\n",
    "    \"Build this function\"\n",
    "    mean_vector = np.mean(Data, axis=0)\n",
    "    centered_data = Data - mean_vector\n",
    "\n",
    "    # Step 2: Calculate the Covariance Matrix\n",
    "    covariance_matrix = np.cov(centered_data, rowvar=False)\n",
    "\n",
    "    # Step 3: Find Eigenvalues and Eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    # Step 4: Sort Eigenvalues and Eigenvectors\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # Step 5: Select the top k eigenvectors as principal components\n",
    "    principal_components = eigenvectors[:, :k]\n",
    "\n",
    "    # Step 6: Project the data onto the selected principal components\n",
    "    X_reduced = np.dot(principal_components.transpose(),centered_data.transpose()).transpose()\n",
    "\n",
    "#     projected_data = np.dot(centered_data, principal_components)\n",
    "\n",
    "    return X_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca(Data , k):\n",
    "     \n",
    "    #Step-1\n",
    "    X_meaned = Data - np.mean(Data , axis = 0)\n",
    "     \n",
    "    #Step-2\n",
    "    cov_mat = np.cov(X_meaned , rowvar = False)\n",
    "     \n",
    "    #Step-3\n",
    "    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "     \n",
    "    #Step-4\n",
    "    sorted_index = np.argsort(eigen_values)[::-1]\n",
    "    sorted_eigenvalue = eigen_values[sorted_index]\n",
    "    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n",
    "     \n",
    "    #Step-5\n",
    "    eigenvector_subset = sorted_eigenvectors[:,0:k]\n",
    "     \n",
    "    #Step-6\n",
    "    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()\n",
    "     \n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read your TrainData and Test data. Try to remove the last column of the training data and assign it to one variable, do same thing for the TestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the code here\n",
    "df_train = pd.read_csv('TrainData.csv')\n",
    "X_train = df_train.iloc[:,:-1]\n",
    "X_test = df_train.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('TestData.csv')\n",
    "y_train = df_test.iloc[:,:-1]\n",
    "y_test = df_test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply my_pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the code here\n",
    "result = my_pca(X_train,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_y = my_pca(y_train,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply pca using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the code here\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "scaling=StandardScaler()\n",
    "Scaled_data=scaling.fit_transform(X_train)\n",
    "Scaled_data_y = scaling.fit_transform(y_train)\n",
    "\n",
    "principal=PCA(n_components=3)\n",
    "\n",
    "x=principal.fit_transform(Scaled_data)\n",
    "y = principal.fit_transform(Scaled_data_y)\n",
    "\n",
    "\n",
    "# plt.figure(figsize = (6,6))\n",
    "# sb.scatterplot(data = principal_df , x = 'PC1',y = 'PC2' , hue = 'T_10305' , s = 60 , palette= 'icefire')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create kernel PCA function that takes two inputs: dataset, and k= number of principle component, and return the transform data. In addition, you need to create three other function one for rbf_kernel, one for polynomial_kernel, and one for linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rbf_kernel(x, y, gamma=1.0):\n",
    "    return np.exp(-gamma * np.linalg.norm(x - y) ** 2)\n",
    "\n",
    "def poly_kernel(x, y, degree=3):\n",
    "    return (np.dot(x, y) + 1) ** degree\n",
    "\n",
    "def linear_kernel(x, y):\n",
    "    \n",
    "    return np.dot(x, y)\n",
    "\n",
    "def my_kpca(data, n_components, kernel_type='rbf', kernel_param=1.0):\n",
    "    \"\"\"\n",
    "    Kernel Principal Component Analysis (KPCA) function.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Input data as an ndarray of shape (n_samples, n_features).\n",
    "    - n_components: Number of principal components to retain.\n",
    "    - kernel_type: Type of kernel ('rbf', 'poly', or 'linear').\n",
    "    - kernel_param: Kernel parameter (e.g., gamma for RBF, degree for polynomial).\n",
    "\n",
    "    Returns:\n",
    "    - Transformed data in the KPCA space.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples, n_features = data.shape\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "\n",
    "    # Calculate the kernel matrix\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            if kernel_type == 'rbf':\n",
    "                K[i, j] = rbf_kernel(data[i], data[j], gamma=kernel_param)\n",
    "            elif kernel_type == 'poly':\n",
    "                K[i, j] = poly_kernel(data[i], data[j], degree=kernel_param)\n",
    "            elif kernel_type == 'linear':\n",
    "                K[i, j] = linear_kernel(data[i], data[j])\n",
    "\n",
    "    # Center the kernel matrix\n",
    "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
    "    K_centered = K - np.dot(one_n, K) - np.dot(K, one_n) + np.dot(np.dot(one_n, K), one_n)\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors of the centered kernel matrix\n",
    "    eigvals, eigvecs = np.linalg.eigh(K_centered)\n",
    "\n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[sorted_indices]\n",
    "    eigvecs = eigvecs[:, sorted_indices]\n",
    "\n",
    "    # Select the top n_components eigenvectors\n",
    "    eigvecs = eigvecs[:, :n_components]\n",
    "\n",
    "    # Project the data into KPCA space\n",
    "    kpca_data = np.dot(K_centered, eigvecs)\n",
    "\n",
    "    return kpca_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply my_kpca on the Train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_kpca = my_kpca(train_x.to_numpy(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Kpca using sklearn on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your code here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import KernelPCA\n",
    "stndS = StandardScaler()\n",
    "XTrain = stndS.fit_transform(train_x)\n",
    "XTest = stndS.fit_transform(test_x)\n",
    "\n",
    "Kernel_pca = KernelPCA(n_components = 2, kernel= \"rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your code her \n",
    "XTrain = Kernel_pca.fit_transform(XTrain)\n",
    "XTest = Kernel_pca.transform(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can use the functions below as your classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distance between two points\n",
    "def dis(x1, x2):\n",
    "    return np.linalg.norm(x1 - x2)\n",
    "\n",
    "# Function to perform classification \n",
    "def myclassifier(Train, Trainlabel, Test):\n",
    "    \" Train is the training data\"\n",
    "    \" Trainlabel is the training labels\"\n",
    "    \" Test is the testing data\"\n",
    "    pred = []\n",
    "\n",
    "    for testpoint in Test:\n",
    "        pred_dis = []\n",
    "        for trainpoint in Train:\n",
    "            pred_dis.append(dis(testpoint, trainpoint))\n",
    "\n",
    "        pred.append(np.argmin(pred_dis))  \n",
    "\n",
    "    return np.array(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below function is to calculte the accuracy , you can use this function to get the accuracy of pca and kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    # Ensure that the true labels and predicted labels have the same length\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Length of true_labels and predicted_labels must be the same.\")\n",
    "\n",
    "    # Count the number of correct predictions\n",
    "    correct_predictions = sum(1 for true, predicted in zip(true_labels, predicted_labels) if true == predicted)\n",
    "\n",
    "    # Calculate accuracy as the ratio of correct predictions to total predictions\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = myclassifier(x,X_test,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code is an implementation of Principal Component Analysis (PCA) from scratch. PCA is a dimensionality reduction technique used to reduce the number of features (variables) in a dataset while preserving as much of the original information as possible. Here's a summary of each step in the code:\n",
    "\n",
    "1. **Mean Centering:**\n",
    "   - `X_meaned = Data - np.mean(Data, axis=0)` subtracts the mean of each feature (column) from the data points. This centers the data around the origin.\n",
    "\n",
    "2. **Covariance Matrix Calculation:**\n",
    "   - `cov_mat = np.cov(X_meaned, rowvar=False)` computes the covariance matrix of the mean-centered data. The `rowvar=False` argument indicates that each column represents a variable, not a row.\n",
    "\n",
    "3. **Eigenvalue and Eigenvector Calculation:**\n",
    "   - `eigen_values, eigen_vectors = np.linalg.eigh(cov_mat)` calculates the eigenvalues and eigenvectors of the covariance matrix. These eigenvalues and eigenvectors represent the directions and magnitudes of maximum variance in the data.\n",
    "\n",
    "4. **Sorting Eigenvalues and Eigenvectors:**\n",
    "   - `sorted_index = np.argsort(eigen_values)[::-1]` sorts the eigenvalues in descending order, and `sorted_eigenvalue` and `sorted_eigenvectors` store them accordingly. This step ensures that the principal components are ranked by their importance.\n",
    "\n",
    "5. **Selecting the Top k Eigenvectors:**\n",
    "   - `eigenvector_subset = sorted_eigenvectors[:, 0:k]` selects the top k eigenvectors, corresponding to the k largest eigenvalues. These eigenvectors will be used as the principal components.\n",
    "\n",
    "6. **Projection:**\n",
    "   - `X_reduced = np.dot(eigenvector_subset.transpose(), X_meaned.transpose()).transpose()` projects the mean-centered data onto the selected principal components. This step reduces the dimensionality of the data while retaining most of the original information.\n",
    "\n",
    "Overall, this code effectively performs PCA and returns the dataset reduced to k dimensions. You can use this function to apply PCA to your data when you specify the input data (`Data`) and the desired number of principal components (`k`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose an appropriate number of principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose an appropriate number of principal components to retain a significant amount of variance in your dataset, you can follow these steps:\n",
    "\n",
    "1. **Perform PCA:** Apply Principal Component Analysis (PCA) to your dataset. This involves mean-centering your data, calculating the covariance matrix, and computing the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "2. **Sort Eigenvalues:** Sort the eigenvalues in descending order. Eigenvalues represent the amount of variance explained by each principal component, with larger eigenvalues explaining more variance.\n",
    "\n",
    "3. **Calculate Explained Variance Ratio:** Calculate the explained variance ratio for each principal component. This is done by dividing each eigenvalue by the sum of all eigenvalues. It represents the proportion of total variance explained by that component.\n",
    "\n",
    "4. **Cumulative Explained Variance:** Calculate the cumulative explained variance by summing the explained variance ratios from the first principal component up to the current component. This tells you how much of the total variance is explained as you add more components.\n",
    "\n",
    "5. **Set a Threshold:** Decide on a threshold for the amount of variance you want to retain. For example, you mentioned retaining a \"significant\" amount, which might be 95% of the variance.\n",
    "\n",
    "6. **Select k Components:** Choose the number of principal components (k) such that the cumulative explained variance exceeds or reaches your chosen threshold. This ensures that you retain a significant amount of variance while reducing the dimensionality of the data.\n",
    "\n",
    "7. **Apply PCA with Selected k:** Finally, apply PCA again, but this time, only retain the top k principal components. These k components capture the most important information in your data while reducing its dimensionality.\n",
    "\n",
    "For example, if you find that the cumulative explained variance reaches or exceeds 95% when you include the first 10 principal components, you can choose k = 10 as an appropriate number of components to retain a significant amount of variance.\n",
    "\n",
    "Keep in mind that the appropriate value of k may vary depending on your specific dataset and the trade-off between dimensionality reduction and retaining information. You can adjust the threshold to meet your specific needs, such as 90%, 95%, or 99% explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retain the significant value of variance upto 95%, value of principal component should be 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
